<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://hanxiaotian.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hanxiaotian.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-19T07:36:07+00:00</updated><id>https://hanxiaotian.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">InfiMM-Eval Benchmark Released</title><link href="https://hanxiaotian.github.io/blog/2023/post-InfiMM-Eval/" rel="alternate" type="text/html" title="InfiMM-Eval Benchmark Released"/><published>2023-12-01T13:56:00+00:00</published><updated>2023-12-01T13:56:00+00:00</updated><id>https://hanxiaotian.github.io/blog/2023/post-InfiMM-Eval</id><content type="html" xml:base="https://hanxiaotian.github.io/blog/2023/post-InfiMM-Eval/"><![CDATA[<p>ğŸ‰ğŸ‰Excited to announce InfiMM-Eval, a benchmark dataset for evaluating reasoning capability of Multimodal LLMs. ğŸ‰ğŸ‰</p> <p>ğŸ¥³Project page: <a href="https://infimm.github.io/InfiMM-Eval/">https://infimm.github.io/InfiMM-Eval/</a></p> <p>ğŸŠGithub repo: <a href="https://infimm.github.io/InfiMM-Eval/">https://infimm.github.io/InfiMM-Eval/</a></p> <p>ğŸ“œArxiv paper: <a href="https://arxiv.org/abs/2311.11567">https://arxiv.org/abs/2311.11567</a></p> <p>ğŸ¤—Huggingface paper: <a href="https://huggingface.co/papers/2311.11567">https://huggingface.co/papers/2311.11567</a></p>]]></content><author><name></name></author><category term="work"/><category term="InfiMM-Eval"/><category term="Benchmark"/><category term="Multimodal"/><category term="LLMs"/><summary type="html"><![CDATA[InfiMM-Eval Benchmark release announcement]]></summary></entry></feed>