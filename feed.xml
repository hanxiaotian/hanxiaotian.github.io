<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://hanxiaotian.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hanxiaotian.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-20T06:24:26+00:00</updated><id>https://hanxiaotian.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Multimodal Large Language Models Sharing Series â€“ 1</title><link href="https://hanxiaotian.github.io/blog/2024/post-MM-Series1/" rel="alternate" type="text/html" title="Multimodal Large Language Models Sharing Series â€“ 1"/><published>2024-01-18T13:56:00+00:00</published><updated>2024-01-18T13:56:00+00:00</updated><id>https://hanxiaotian.github.io/blog/2024/post-MM-Series1</id><content type="html" xml:base="https://hanxiaotian.github.io/blog/2024/post-MM-Series1/"><![CDATA[<p>ğŸ‰ğŸ‰ Happy to share some of our recent work about Multimodal Large Language Models. ğŸ‰ğŸ‰</p> <h3 id="1ï¸âƒ£-a-survey-for-multimodal-reasoning">1ï¸âƒ£ A Survey for Multimodal Reasoning</h3> <p>Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoningintensive tasks, and finally discuss current practices and future directions.</p> <p><img src="/assets/img/publication_preview/MLLM_reasoning_tree.png" style="zoom:40%;"/></p> <p>For more details, please refer to our ğŸ“œpaper: <a href="https://arxiv.org/abs/2401.06805">https://arxiv.org/abs/2401.06805</a></p> <h3 id="2ï¸âƒ£-advancing-flamingo-through-diverse-llm-integration">2ï¸âƒ£ Advancing Flamingo through Diverse LLM Integration</h3> <p>Inspired by Flamingo, we present InfiMM model series. This is another reproduction for Flamingo, with stronger LLMs (e.g. <a href="https://ai.meta.com/llama/">LLaMA2-13B</a>, <a href="https://huggingface.co/lmsys/vicuna-13b-v1.5">Vicuna-13B</a>, <a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta">Zephyr7B</a>), better filtering of pre-training data, and more carefully designed instruction fine-tuning. Compared with previous open-sourced attempts (<a href="https://github.com/mlfoundations/open_flamingo">OpenFlamingo</a> and <a href="https://huggingface.co/blog/idefics">IDEFIC</a>), InfiMM achieves better performance on recent benchmarks (e.g., MMMU, InfiMM-Eval, MM-Vet, etc).</p> <p>We achieved top performance on MMMU open-source MLLMs:</p> <p><img src="https://huggingface.co/Infi-MM/infimm-zephyr/resolve/main/assets/infimm-zephyr-mmmu-val.jpeg" style="zoom:40%;"/></p> <p>For details about our model, please refer to ğŸ¤—Huggingface: <a href="https://huggingface.co/Infi-MM/infimm-zephyr">Infi-MM/infimm-zephyr</a></p> <h3 id="3ï¸âƒ£-exploring-multimodal-instruction-fine-tuning">3ï¸âƒ£ Exploring Multimodal Instruction Fine-tuning</h3> <p>Visual instruction fine-tuning (IFT) is a vital process for aligning MLLMsâ€™ output with userâ€™s intentions. We noticed that models trained with LLaVA-mix-665k dataset often struggle to follow user instructions properly in multi-round dialog. We argue that datasets with diverse and high-quality detailed instruction following annotations are essential and adequate for MLLMs IFT. In this work, we establish a new IFT dataset, with images sourced from the COCO dataset only, along with more diverse instructions. Our experiments show that when fine-tuned with our proposed dataset, MLLMs achieve better performance on open-ended evaluation benchmarks in both single-round and multi-round dialog setting.</p> <p><img src="/assets/img/publication_preview/coco_all_you_need.png" style="zoom:40%;"/></p> <p>For more detals, please refer to our ğŸ“œpaper: <a href="https://arxiv.org/abs/2401.08968">https://arxiv.org/abs/2401.08968</a></p>]]></content><author><name></name></author><category term="work"/><category term="Reasoning"/><category term="Multimodal Language Models"/><summary type="html"><![CDATA[Posts about Multimodal Reasoning]]></summary></entry><entry><title type="html">InfiMM-Eval Benchmark Released</title><link href="https://hanxiaotian.github.io/blog/2023/post-InfiMM-Eval/" rel="alternate" type="text/html" title="InfiMM-Eval Benchmark Released"/><published>2023-12-01T13:56:00+00:00</published><updated>2023-12-01T13:56:00+00:00</updated><id>https://hanxiaotian.github.io/blog/2023/post-InfiMM-Eval</id><content type="html" xml:base="https://hanxiaotian.github.io/blog/2023/post-InfiMM-Eval/"><![CDATA[<p>ğŸ‰ğŸ‰Excited to announce InfiMM-Eval, a benchmark dataset for evaluating reasoning capability of Multimodal LLMs. ğŸ‰ğŸ‰</p> <p>ğŸ¥³Project page: <a href="https://infimm.github.io/InfiMM-Eval/">https://infimm.github.io/InfiMM-Eval/</a></p> <p>ğŸŠGithub repo: <a href="https://infimm.github.io/InfiMM-Eval/">https://infimm.github.io/InfiMM-Eval/</a></p> <p>ğŸ“œArxiv paper: <a href="https://arxiv.org/abs/2311.11567">https://arxiv.org/abs/2311.11567</a></p> <p>ğŸ¤—Huggingface paper: <a href="https://huggingface.co/papers/2311.11567">https://huggingface.co/papers/2311.11567</a></p>]]></content><author><name></name></author><category term="work"/><category term="InfiMM-Eval"/><category term="Benchmark"/><category term="Multimodal"/><category term="LLMs"/><summary type="html"><![CDATA[InfiMM-Eval Benchmark release announcement]]></summary></entry></feed>